{"cells":[{"cell_type":"markdown","metadata":{"id":"CADPrd7MR6c4"},"source":["# MobileViT: A mobile-friendly Transformer-based model for image classification\n","\n","**Author:** [Sayak Paul](https://twitter.com/RisingSayak)<br>\n","**Date created:** 2021/10/20<br>\n","**Last modified:** 2021/10/20<br>\n","**Description:** MobileViT for image classification with combined benefits of convolutions and Transformers."]},{"cell_type":"markdown","metadata":{"id":"O5CEI_z4R6dU"},"source":["## Introduction\n","\n","In this example, we implement the MobileViT architecture\n","([Mehta et al.](https://arxiv.org/abs/2110.02178)),\n","which combines the benefits of Transformers\n","([Vaswani et al.](https://arxiv.org/abs/1706.03762))\n","and convolutions. With Transformers, we can capture long-range dependencies that result\n","in global representations. With convolutions, we can capture spatial relationships that\n","model locality.\n","\n","Besides combining the properties of Transformers and convolutions, the authors introduce\n","MobileViT as a general-purpose mobile-friendly backbone for different image recognition\n","tasks. Their findings suggest that, performance-wise, MobileViT is better than other\n","models with the same or higher complexity ([MobileNetV3](https://arxiv.org/abs/1905.02244),\n","for example), while being efficient on mobile devices."]},{"cell_type":"markdown","metadata":{"id":"h5U9AwmhR6da"},"source":["## Imports"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aq22TS6wSqWb","outputId":"5f48224e-1651-4a1d-807d-9c3e7b643b68","executionInfo":{"status":"ok","timestamp":1658307150773,"user_tz":-120,"elapsed":138768,"user":{"displayName":"Sagnik Gupta","userId":"06692511853419568091"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting tensorflow==2.8\n","  Downloading https://us-python.pkg.dev/colab-wheels/public/tensorflow/tensorflow-2.8.0%2Bzzzcolab20220506162203-cp37-cp37m-linux_x86_64.whl (668.3 MB)\n","\u001b[K     |████████████████████████████████| 668.3 MB 17 kB/s \n","\u001b[?25hRequirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (0.5.3)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (3.1.0)\n","Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (3.17.3)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (1.14.1)\n","Collecting tf-estimator-nightly==2.8.0.dev2021122109\n","  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n","\u001b[K     |████████████████████████████████| 462 kB 5.1 MB/s \n","\u001b[?25hRequirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (2.0)\n","Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (2.8.0)\n","Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (1.1.2)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (1.6.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (57.4.0)\n","Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (14.0.1)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (1.1.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (0.2.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (1.47.0)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (3.3.0)\n","Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (1.1.0)\n","Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (2.8.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (4.1.1)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (1.15.0)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (0.26.0)\n","Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (1.21.6)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow==2.8) (0.37.1)\n","Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow==2.8) (1.5.2)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (1.8.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (3.3.7)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (2.23.0)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (1.35.0)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (0.4.6)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (1.0.1)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (0.6.1)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8) (4.8)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8) (0.2.8)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8) (4.2.4)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8) (1.3.1)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow==2.8) (4.12.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow==2.8) (3.8.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8) (0.4.8)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8) (2022.6.15)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8) (1.24.3)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8) (3.2.0)\n","Installing collected packages: tf-estimator-nightly, tensorflow\n","  Attempting uninstall: tensorflow\n","    Found existing installation: tensorflow 2.8.2+zzzcolab20220527125636\n","    Uninstalling tensorflow-2.8.2+zzzcolab20220527125636:\n","      Successfully uninstalled tensorflow-2.8.2+zzzcolab20220527125636\n","Successfully installed tensorflow-2.8.0+zzzcolab20220506162203 tf-estimator-nightly-2.8.0.dev2021122109\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","The following package was automatically installed and is no longer required:\n","  libnvidia-common-460\n","Use 'apt autoremove' to remove it.\n","The following packages will be REMOVED:\n","  libcudnn8-dev\n","The following held packages will be changed:\n","  libcudnn8\n","The following packages will be upgraded:\n","  libcudnn8\n","1 upgraded, 0 newly installed, 1 to remove and 47 not upgraded.\n","Need to get 430 MB of archives.\n","After this operation, 3,139 MB disk space will be freed.\n","Get:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  libcudnn8 8.1.0.77-1+cuda11.2 [430 MB]\n","Fetched 430 MB in 7s (62.1 MB/s)\n","(Reading database ... 155653 files and directories currently installed.)\n","Removing libcudnn8-dev (8.0.5.39-1+cuda11.1) ...\n","(Reading database ... 155631 files and directories currently installed.)\n","Preparing to unpack .../libcudnn8_8.1.0.77-1+cuda11.2_amd64.deb ...\n","Unpacking libcudnn8 (8.1.0.77-1+cuda11.2) over (8.0.5.39-1+cuda11.1) ...\n","Setting up libcudnn8 (8.1.0.77-1+cuda11.2) ...\n"]}],"source":["# !pip install -q tensorflow-addons==0.8.3\n","# !pip install -q tensorflow==2.9.1\n","!pip install tensorflow==2.8\n","!apt install --allow-change-held-packages libcudnn8=8.1.0.77-1+cuda11.2"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"rNDfHb4XEmmo","executionInfo":{"status":"ok","timestamp":1658307150774,"user_tz":-120,"elapsed":13,"user":{"displayName":"Sagnik Gupta","userId":"06692511853419568091"}}},"outputs":[],"source":["# !pip install --upgrade tensorflow\n","# !pip install --upgrade tensorflow-gpu"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"rMKiqyKyR6dc","executionInfo":{"status":"ok","timestamp":1658307154268,"user_tz":-120,"elapsed":3506,"user":{"displayName":"Sagnik Gupta","userId":"06692511853419568091"}}},"outputs":[],"source":["import tensorflow as tf\n","\n","from keras.applications import imagenet_utils\n","from keras import layers\n","from tensorflow import keras\n","import numpy as np\n","import sys\n","\n","import tensorflow_datasets as tfds\n","#import tensorflow_addons as tfa\n","\n","tfds.disable_progress_bar()"]},{"cell_type":"markdown","metadata":{"id":"M8m-WCW-R6dl"},"source":["## Hyperparameters"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"DL0SvpWZR6do","executionInfo":{"status":"ok","timestamp":1658307154269,"user_tz":-120,"elapsed":5,"user":{"displayName":"Sagnik Gupta","userId":"06692511853419568091"}}},"outputs":[],"source":["# Values are from table 4.\n","patch_size = 4  # 2x2, for the Transformer blocks.\n","image_size = 256\n","expansion_factor = 2  # expansion factor for the MobileNetV2 blocks."]},{"cell_type":"markdown","metadata":{"id":"9HXKRRHzR6du"},"source":["## MobileViT utilities\n","\n","The MobileViT architecture is comprised of the following blocks:\n","\n","* Strided 3x3 convolutions that process the input image.\n","* [MobileNetV2](https://arxiv.org/abs/1801.04381)-style inverted residual blocks for\n","downsampling the resolution of the intermediate feature maps.\n","* MobileViT blocks that combine the benefits of Transformers and convolutions. It is\n","presented in the figure below (taken from the\n","[original paper](https://arxiv.org/abs/2110.02178)):\n","\n","\n","![](https://i.imgur.com/mANnhI7.png)"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"Zfu2o2a2R6d1","executionInfo":{"status":"ok","timestamp":1658307154269,"user_tz":-120,"elapsed":4,"user":{"displayName":"Sagnik Gupta","userId":"06692511853419568091"}}},"outputs":[],"source":["\n","def conv_block(x, filters=16, kernel_size=3, strides=2):\n","    conv_layer = layers.Conv2D(\n","        filters, kernel_size, strides=strides, activation=tf.nn.swish, padding=\"same\"\n","    )\n","    return conv_layer(x)\n","\n","\n","# Reference: https://git.io/JKgtC\n","\n","\n","def inverted_residual_block(x, expanded_channels, output_channels, strides=1):\n","    m = layers.Conv2D(expanded_channels, 1, padding=\"same\", use_bias=False)(x)\n","    m = layers.BatchNormalization()(m)\n","    m = tf.nn.swish(m)\n","\n","    if strides == 2:\n","        m = layers.ZeroPadding2D(padding=imagenet_utils.correct_pad(m, 3))(m)\n","    m = layers.DepthwiseConv2D(\n","        3, strides=strides, padding=\"same\" if strides == 1 else \"valid\", use_bias=False\n","    )(m)\n","    m = layers.BatchNormalization()(m)\n","    m = tf.nn.swish(m)\n","\n","    m = layers.Conv2D(output_channels, 1, padding=\"same\", use_bias=False)(m)\n","    m = layers.BatchNormalization()(m)\n","\n","    if tf.math.equal(x.shape[-1], output_channels) and strides == 1:\n","        return layers.Add()([m, x])\n","    return m\n","\n","\n","# Reference:\n","# https://keras.io/examples/vision/image_classification_with_vision_transformer/\n","\n","\n","def mlp(x, hidden_units, dropout_rate):\n","    for units in hidden_units:\n","        x = layers.Dense(units, activation=tf.nn.swish)(x)\n","        x = layers.Dropout(dropout_rate)(x)\n","    return x\n","\n","\n","def transformer_block(x, transformer_layers, projection_dim, num_heads=2):\n","    for _ in range(transformer_layers):\n","        # Layer normalization 1.\n","        x1 = layers.LayerNormalization(epsilon=1e-6)(x)\n","        # Create a multi-head attention layer.\n","        attention_output = layers.MultiHeadAttention(\n","            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n","        )(x1, x1)\n","        # Skip connection 1.\n","        x2 = layers.Add()([attention_output, x])\n","        # Layer normalization 2.\n","        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n","        # MLP.\n","        x3 = mlp(x3, hidden_units=[x.shape[-1] * 2, x.shape[-1]], dropout_rate=0.1,)\n","        # Skip connection 2.\n","        x = layers.Add()([x3, x2])\n","\n","    return x\n","\n","\n","def mobilevit_block(x, num_blocks, projection_dim, strides=1):\n","    # Local projection with convolutions.\n","    local_features = conv_block(x, filters=projection_dim, strides=strides)\n","    local_features = conv_block(\n","        local_features, filters=projection_dim, kernel_size=1, strides=strides\n","    )\n","\n","    # Unfold into patches and then pass through Transformers.\n","    num_patches = int((local_features.shape[1] * local_features.shape[2]) / patch_size)\n","    non_overlapping_patches = layers.Reshape((patch_size, num_patches, projection_dim))(\n","        local_features\n","    )\n","    global_features = transformer_block(\n","        non_overlapping_patches, num_blocks, projection_dim\n","    )\n","\n","    # Fold into conv-like feature-maps.\n","    folded_feature_map = layers.Reshape((*local_features.shape[1:-1], projection_dim))(\n","        global_features\n","    )\n","\n","    # Apply point-wise conv -> concatenate with the input features.\n","    folded_feature_map = conv_block(\n","        folded_feature_map, filters=x.shape[-1], kernel_size=1, strides=strides\n","    )\n","    local_global_features = layers.Concatenate(axis=-1)([x, folded_feature_map])\n","\n","    # Fuse the local and global features using a convoluion layer.\n","    local_global_features = conv_block(\n","        local_global_features, filters=projection_dim, strides=strides\n","    )\n","\n","    return local_global_features\n"]},{"cell_type":"markdown","metadata":{"id":"iqCpE8zoR6eD"},"source":["**More on the MobileViT block**:\n","\n","* First, the feature representations (A) go through convolution blocks that capture local\n","relationships. The expected shape of a single entry here would be `(h, w, num_channels)`.\n","* Then they get unfolded into another vector with shape `(p, n, num_channels)`,\n","where `p` is the area of a small patch, and `n` is `(h * w) / p`. So, we end up with `n`\n","non-overlapping patches.\n","* This unfolded vector is then passed through a Tranformer block that captures global\n","relationships between the patches.\n","* The output vector (B) is again folded into a vector of shape `(h, w, num_channels)`\n","resembling a feature map coming out of convolutions.\n","\n","Vectors A and B are then passed through two more convolutional layers to fuse the local\n","and global representations. Notice how the spatial resolution of the final vector remains\n","unchanged at this point. The authors also present an explanation of how the MobileViT\n","block resembles a convolution block of a CNN. For more details, please refer to the\n","original paper."]},{"cell_type":"markdown","metadata":{"id":"rhtHjHeTR6eL"},"source":["Next, we combine these blocks together and implement the MobileViT architecture (XXS\n","variant). The following figure (taken from the original paper) presents a schematic\n","representation of the architecture:\n","\n","![](https://i.ibb.co/sRbVRBN/image.png)"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ghmz9OHfR6eN","outputId":"ef3b2301-103c-427e-e1b7-5c52b0421b7f","executionInfo":{"status":"ok","timestamp":1658307159861,"user_tz":-120,"elapsed":5596,"user":{"displayName":"Sagnik Gupta","userId":"06692511853419568091"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_1 (InputLayer)           [(None, 256, 256, 3  0           []                               \n","                                )]                                                                \n","                                                                                                  \n"," rescaling (Rescaling)          (None, 256, 256, 3)  0           ['input_1[0][0]']                \n","                                                                                                  \n"," conv2d (Conv2D)                (None, 128, 128, 16  448         ['rescaling[0][0]']              \n","                                )                                                                 \n","                                                                                                  \n"," conv2d_1 (Conv2D)              (None, 128, 128, 32  512         ['conv2d[0][0]']                 \n","                                )                                                                 \n","                                                                                                  \n"," batch_normalization (BatchNorm  (None, 128, 128, 32  128        ['conv2d_1[0][0]']               \n"," alization)                     )                                                                 \n","                                                                                                  \n"," tf.nn.silu (TFOpLambda)        (None, 128, 128, 32  0           ['batch_normalization[0][0]']    \n","                                )                                                                 \n","                                                                                                  \n"," depthwise_conv2d (DepthwiseCon  (None, 128, 128, 32  288        ['tf.nn.silu[0][0]']             \n"," v2D)                           )                                                                 \n","                                                                                                  \n"," batch_normalization_1 (BatchNo  (None, 128, 128, 32  128        ['depthwise_conv2d[0][0]']       \n"," rmalization)                   )                                                                 \n","                                                                                                  \n"," tf.nn.silu_1 (TFOpLambda)      (None, 128, 128, 32  0           ['batch_normalization_1[0][0]']  \n","                                )                                                                 \n","                                                                                                  \n"," conv2d_2 (Conv2D)              (None, 128, 128, 16  512         ['tf.nn.silu_1[0][0]']           \n","                                )                                                                 \n","                                                                                                  \n"," batch_normalization_2 (BatchNo  (None, 128, 128, 16  64         ['conv2d_2[0][0]']               \n"," rmalization)                   )                                                                 \n","                                                                                                  \n"," add (Add)                      (None, 128, 128, 16  0           ['batch_normalization_2[0][0]',  \n","                                )                                 'conv2d[0][0]']                 \n","                                                                                                  \n"," conv2d_3 (Conv2D)              (None, 128, 128, 32  512         ['add[0][0]']                    \n","                                )                                                                 \n","                                                                                                  \n"," batch_normalization_3 (BatchNo  (None, 128, 128, 32  128        ['conv2d_3[0][0]']               \n"," rmalization)                   )                                                                 \n","                                                                                                  \n"," tf.nn.silu_2 (TFOpLambda)      (None, 128, 128, 32  0           ['batch_normalization_3[0][0]']  \n","                                )                                                                 \n","                                                                                                  \n"," zero_padding2d (ZeroPadding2D)  (None, 129, 129, 32  0          ['tf.nn.silu_2[0][0]']           \n","                                )                                                                 \n","                                                                                                  \n"," depthwise_conv2d_1 (DepthwiseC  (None, 64, 64, 32)  288         ['zero_padding2d[0][0]']         \n"," onv2D)                                                                                           \n","                                                                                                  \n"," batch_normalization_4 (BatchNo  (None, 64, 64, 32)  128         ['depthwise_conv2d_1[0][0]']     \n"," rmalization)                                                                                     \n","                                                                                                  \n"," tf.nn.silu_3 (TFOpLambda)      (None, 64, 64, 32)   0           ['batch_normalization_4[0][0]']  \n","                                                                                                  \n"," conv2d_4 (Conv2D)              (None, 64, 64, 24)   768         ['tf.nn.silu_3[0][0]']           \n","                                                                                                  \n"," batch_normalization_5 (BatchNo  (None, 64, 64, 24)  96          ['conv2d_4[0][0]']               \n"," rmalization)                                                                                     \n","                                                                                                  \n"," conv2d_5 (Conv2D)              (None, 64, 64, 48)   1152        ['batch_normalization_5[0][0]']  \n","                                                                                                  \n"," batch_normalization_6 (BatchNo  (None, 64, 64, 48)  192         ['conv2d_5[0][0]']               \n"," rmalization)                                                                                     \n","                                                                                                  \n"," tf.nn.silu_4 (TFOpLambda)      (None, 64, 64, 48)   0           ['batch_normalization_6[0][0]']  \n","                                                                                                  \n"," depthwise_conv2d_2 (DepthwiseC  (None, 64, 64, 48)  432         ['tf.nn.silu_4[0][0]']           \n"," onv2D)                                                                                           \n","                                                                                                  \n"," batch_normalization_7 (BatchNo  (None, 64, 64, 48)  192         ['depthwise_conv2d_2[0][0]']     \n"," rmalization)                                                                                     \n","                                                                                                  \n"," tf.nn.silu_5 (TFOpLambda)      (None, 64, 64, 48)   0           ['batch_normalization_7[0][0]']  \n","                                                                                                  \n"," conv2d_6 (Conv2D)              (None, 64, 64, 24)   1152        ['tf.nn.silu_5[0][0]']           \n","                                                                                                  \n"," batch_normalization_8 (BatchNo  (None, 64, 64, 24)  96          ['conv2d_6[0][0]']               \n"," rmalization)                                                                                     \n","                                                                                                  \n"," add_1 (Add)                    (None, 64, 64, 24)   0           ['batch_normalization_8[0][0]',  \n","                                                                  'batch_normalization_5[0][0]']  \n","                                                                                                  \n"," conv2d_7 (Conv2D)              (None, 64, 64, 48)   1152        ['add_1[0][0]']                  \n","                                                                                                  \n"," batch_normalization_9 (BatchNo  (None, 64, 64, 48)  192         ['conv2d_7[0][0]']               \n"," rmalization)                                                                                     \n","                                                                                                  \n"," tf.nn.silu_6 (TFOpLambda)      (None, 64, 64, 48)   0           ['batch_normalization_9[0][0]']  \n","                                                                                                  \n"," depthwise_conv2d_3 (DepthwiseC  (None, 64, 64, 48)  432         ['tf.nn.silu_6[0][0]']           \n"," onv2D)                                                                                           \n","                                                                                                  \n"," batch_normalization_10 (BatchN  (None, 64, 64, 48)  192         ['depthwise_conv2d_3[0][0]']     \n"," ormalization)                                                                                    \n","                                                                                                  \n"," tf.nn.silu_7 (TFOpLambda)      (None, 64, 64, 48)   0           ['batch_normalization_10[0][0]'] \n","                                                                                                  \n"," conv2d_8 (Conv2D)              (None, 64, 64, 24)   1152        ['tf.nn.silu_7[0][0]']           \n","                                                                                                  \n"," batch_normalization_11 (BatchN  (None, 64, 64, 24)  96          ['conv2d_8[0][0]']               \n"," ormalization)                                                                                    \n","                                                                                                  \n"," add_2 (Add)                    (None, 64, 64, 24)   0           ['batch_normalization_11[0][0]', \n","                                                                  'add_1[0][0]']                  \n","                                                                                                  \n"," conv2d_9 (Conv2D)              (None, 64, 64, 48)   1152        ['add_2[0][0]']                  \n","                                                                                                  \n"," batch_normalization_12 (BatchN  (None, 64, 64, 48)  192         ['conv2d_9[0][0]']               \n"," ormalization)                                                                                    \n","                                                                                                  \n"," tf.nn.silu_8 (TFOpLambda)      (None, 64, 64, 48)   0           ['batch_normalization_12[0][0]'] \n","                                                                                                  \n"," zero_padding2d_1 (ZeroPadding2  (None, 65, 65, 48)  0           ['tf.nn.silu_8[0][0]']           \n"," D)                                                                                               \n","                                                                                                  \n"," depthwise_conv2d_4 (DepthwiseC  (None, 32, 32, 48)  432         ['zero_padding2d_1[0][0]']       \n"," onv2D)                                                                                           \n","                                                                                                  \n"," batch_normalization_13 (BatchN  (None, 32, 32, 48)  192         ['depthwise_conv2d_4[0][0]']     \n"," ormalization)                                                                                    \n","                                                                                                  \n"," tf.nn.silu_9 (TFOpLambda)      (None, 32, 32, 48)   0           ['batch_normalization_13[0][0]'] \n","                                                                                                  \n"," conv2d_10 (Conv2D)             (None, 32, 32, 48)   2304        ['tf.nn.silu_9[0][0]']           \n","                                                                                                  \n"," batch_normalization_14 (BatchN  (None, 32, 32, 48)  192         ['conv2d_10[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," conv2d_11 (Conv2D)             (None, 32, 32, 64)   27712       ['batch_normalization_14[0][0]'] \n","                                                                                                  \n"," conv2d_12 (Conv2D)             (None, 32, 32, 64)   4160        ['conv2d_11[0][0]']              \n","                                                                                                  \n"," reshape (Reshape)              (None, 4, 256, 64)   0           ['conv2d_12[0][0]']              \n","                                                                                                  \n"," layer_normalization (LayerNorm  (None, 4, 256, 64)  128         ['reshape[0][0]']                \n"," alization)                                                                                       \n","                                                                                                  \n"," multi_head_attention (MultiHea  (None, 4, 256, 64)  33216       ['layer_normalization[0][0]',    \n"," dAttention)                                                      'layer_normalization[0][0]']    \n","                                                                                                  \n"," add_3 (Add)                    (None, 4, 256, 64)   0           ['multi_head_attention[0][0]',   \n","                                                                  'reshape[0][0]']                \n","                                                                                                  \n"," layer_normalization_1 (LayerNo  (None, 4, 256, 64)  128         ['add_3[0][0]']                  \n"," rmalization)                                                                                     \n","                                                                                                  \n"," dense (Dense)                  (None, 4, 256, 128)  8320        ['layer_normalization_1[0][0]']  \n","                                                                                                  \n"," dropout (Dropout)              (None, 4, 256, 128)  0           ['dense[0][0]']                  \n","                                                                                                  \n"," dense_1 (Dense)                (None, 4, 256, 64)   8256        ['dropout[0][0]']                \n","                                                                                                  \n"," dropout_1 (Dropout)            (None, 4, 256, 64)   0           ['dense_1[0][0]']                \n","                                                                                                  \n"," add_4 (Add)                    (None, 4, 256, 64)   0           ['dropout_1[0][0]',              \n","                                                                  'add_3[0][0]']                  \n","                                                                                                  \n"," layer_normalization_2 (LayerNo  (None, 4, 256, 64)  128         ['add_4[0][0]']                  \n"," rmalization)                                                                                     \n","                                                                                                  \n"," multi_head_attention_1 (MultiH  (None, 4, 256, 64)  33216       ['layer_normalization_2[0][0]',  \n"," eadAttention)                                                    'layer_normalization_2[0][0]']  \n","                                                                                                  \n"," add_5 (Add)                    (None, 4, 256, 64)   0           ['multi_head_attention_1[0][0]', \n","                                                                  'add_4[0][0]']                  \n","                                                                                                  \n"," layer_normalization_3 (LayerNo  (None, 4, 256, 64)  128         ['add_5[0][0]']                  \n"," rmalization)                                                                                     \n","                                                                                                  \n"," dense_2 (Dense)                (None, 4, 256, 128)  8320        ['layer_normalization_3[0][0]']  \n","                                                                                                  \n"," dropout_2 (Dropout)            (None, 4, 256, 128)  0           ['dense_2[0][0]']                \n","                                                                                                  \n"," dense_3 (Dense)                (None, 4, 256, 64)   8256        ['dropout_2[0][0]']              \n","                                                                                                  \n"," dropout_3 (Dropout)            (None, 4, 256, 64)   0           ['dense_3[0][0]']                \n","                                                                                                  \n"," add_6 (Add)                    (None, 4, 256, 64)   0           ['dropout_3[0][0]',              \n","                                                                  'add_5[0][0]']                  \n","                                                                                                  \n"," reshape_1 (Reshape)            (None, 32, 32, 64)   0           ['add_6[0][0]']                  \n","                                                                                                  \n"," conv2d_13 (Conv2D)             (None, 32, 32, 48)   3120        ['reshape_1[0][0]']              \n","                                                                                                  \n"," concatenate (Concatenate)      (None, 32, 32, 96)   0           ['batch_normalization_14[0][0]', \n","                                                                  'conv2d_13[0][0]']              \n","                                                                                                  \n"," conv2d_14 (Conv2D)             (None, 32, 32, 64)   55360       ['concatenate[0][0]']            \n","                                                                                                  \n"," conv2d_15 (Conv2D)             (None, 32, 32, 128)  8192        ['conv2d_14[0][0]']              \n","                                                                                                  \n"," batch_normalization_15 (BatchN  (None, 32, 32, 128)  512        ['conv2d_15[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," tf.nn.silu_10 (TFOpLambda)     (None, 32, 32, 128)  0           ['batch_normalization_15[0][0]'] \n","                                                                                                  \n"," zero_padding2d_2 (ZeroPadding2  (None, 33, 33, 128)  0          ['tf.nn.silu_10[0][0]']          \n"," D)                                                                                               \n","                                                                                                  \n"," depthwise_conv2d_5 (DepthwiseC  (None, 16, 16, 128)  1152       ['zero_padding2d_2[0][0]']       \n"," onv2D)                                                                                           \n","                                                                                                  \n"," batch_normalization_16 (BatchN  (None, 16, 16, 128)  512        ['depthwise_conv2d_5[0][0]']     \n"," ormalization)                                                                                    \n","                                                                                                  \n"," tf.nn.silu_11 (TFOpLambda)     (None, 16, 16, 128)  0           ['batch_normalization_16[0][0]'] \n","                                                                                                  \n"," conv2d_16 (Conv2D)             (None, 16, 16, 64)   8192        ['tf.nn.silu_11[0][0]']          \n","                                                                                                  \n"," batch_normalization_17 (BatchN  (None, 16, 16, 64)  256         ['conv2d_16[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," conv2d_17 (Conv2D)             (None, 16, 16, 80)   46160       ['batch_normalization_17[0][0]'] \n","                                                                                                  \n"," conv2d_18 (Conv2D)             (None, 16, 16, 80)   6480        ['conv2d_17[0][0]']              \n","                                                                                                  \n"," reshape_2 (Reshape)            (None, 4, 64, 80)    0           ['conv2d_18[0][0]']              \n","                                                                                                  \n"," layer_normalization_4 (LayerNo  (None, 4, 64, 80)   160         ['reshape_2[0][0]']              \n"," rmalization)                                                                                     \n","                                                                                                  \n"," multi_head_attention_2 (MultiH  (None, 4, 64, 80)   51760       ['layer_normalization_4[0][0]',  \n"," eadAttention)                                                    'layer_normalization_4[0][0]']  \n","                                                                                                  \n"," add_7 (Add)                    (None, 4, 64, 80)    0           ['multi_head_attention_2[0][0]', \n","                                                                  'reshape_2[0][0]']              \n","                                                                                                  \n"," layer_normalization_5 (LayerNo  (None, 4, 64, 80)   160         ['add_7[0][0]']                  \n"," rmalization)                                                                                     \n","                                                                                                  \n"," dense_4 (Dense)                (None, 4, 64, 160)   12960       ['layer_normalization_5[0][0]']  \n","                                                                                                  \n"," dropout_4 (Dropout)            (None, 4, 64, 160)   0           ['dense_4[0][0]']                \n","                                                                                                  \n"," dense_5 (Dense)                (None, 4, 64, 80)    12880       ['dropout_4[0][0]']              \n","                                                                                                  \n"," dropout_5 (Dropout)            (None, 4, 64, 80)    0           ['dense_5[0][0]']                \n","                                                                                                  \n"," add_8 (Add)                    (None, 4, 64, 80)    0           ['dropout_5[0][0]',              \n","                                                                  'add_7[0][0]']                  \n","                                                                                                  \n"," layer_normalization_6 (LayerNo  (None, 4, 64, 80)   160         ['add_8[0][0]']                  \n"," rmalization)                                                                                     \n","                                                                                                  \n"," multi_head_attention_3 (MultiH  (None, 4, 64, 80)   51760       ['layer_normalization_6[0][0]',  \n"," eadAttention)                                                    'layer_normalization_6[0][0]']  \n","                                                                                                  \n"," add_9 (Add)                    (None, 4, 64, 80)    0           ['multi_head_attention_3[0][0]', \n","                                                                  'add_8[0][0]']                  \n","                                                                                                  \n"," layer_normalization_7 (LayerNo  (None, 4, 64, 80)   160         ['add_9[0][0]']                  \n"," rmalization)                                                                                     \n","                                                                                                  \n"," dense_6 (Dense)                (None, 4, 64, 160)   12960       ['layer_normalization_7[0][0]']  \n","                                                                                                  \n"," dropout_6 (Dropout)            (None, 4, 64, 160)   0           ['dense_6[0][0]']                \n","                                                                                                  \n"," dense_7 (Dense)                (None, 4, 64, 80)    12880       ['dropout_6[0][0]']              \n","                                                                                                  \n"," dropout_7 (Dropout)            (None, 4, 64, 80)    0           ['dense_7[0][0]']                \n","                                                                                                  \n"," add_10 (Add)                   (None, 4, 64, 80)    0           ['dropout_7[0][0]',              \n","                                                                  'add_9[0][0]']                  \n","                                                                                                  \n"," layer_normalization_8 (LayerNo  (None, 4, 64, 80)   160         ['add_10[0][0]']                 \n"," rmalization)                                                                                     \n","                                                                                                  \n"," multi_head_attention_4 (MultiH  (None, 4, 64, 80)   51760       ['layer_normalization_8[0][0]',  \n"," eadAttention)                                                    'layer_normalization_8[0][0]']  \n","                                                                                                  \n"," add_11 (Add)                   (None, 4, 64, 80)    0           ['multi_head_attention_4[0][0]', \n","                                                                  'add_10[0][0]']                 \n","                                                                                                  \n"," layer_normalization_9 (LayerNo  (None, 4, 64, 80)   160         ['add_11[0][0]']                 \n"," rmalization)                                                                                     \n","                                                                                                  \n"," dense_8 (Dense)                (None, 4, 64, 160)   12960       ['layer_normalization_9[0][0]']  \n","                                                                                                  \n"," dropout_8 (Dropout)            (None, 4, 64, 160)   0           ['dense_8[0][0]']                \n","                                                                                                  \n"," dense_9 (Dense)                (None, 4, 64, 80)    12880       ['dropout_8[0][0]']              \n","                                                                                                  \n"," dropout_9 (Dropout)            (None, 4, 64, 80)    0           ['dense_9[0][0]']                \n","                                                                                                  \n"," add_12 (Add)                   (None, 4, 64, 80)    0           ['dropout_9[0][0]',              \n","                                                                  'add_11[0][0]']                 \n","                                                                                                  \n"," layer_normalization_10 (LayerN  (None, 4, 64, 80)   160         ['add_12[0][0]']                 \n"," ormalization)                                                                                    \n","                                                                                                  \n"," multi_head_attention_5 (MultiH  (None, 4, 64, 80)   51760       ['layer_normalization_10[0][0]', \n"," eadAttention)                                                    'layer_normalization_10[0][0]'] \n","                                                                                                  \n"," add_13 (Add)                   (None, 4, 64, 80)    0           ['multi_head_attention_5[0][0]', \n","                                                                  'add_12[0][0]']                 \n","                                                                                                  \n"," layer_normalization_11 (LayerN  (None, 4, 64, 80)   160         ['add_13[0][0]']                 \n"," ormalization)                                                                                    \n","                                                                                                  \n"," dense_10 (Dense)               (None, 4, 64, 160)   12960       ['layer_normalization_11[0][0]'] \n","                                                                                                  \n"," dropout_10 (Dropout)           (None, 4, 64, 160)   0           ['dense_10[0][0]']               \n","                                                                                                  \n"," dense_11 (Dense)               (None, 4, 64, 80)    12880       ['dropout_10[0][0]']             \n","                                                                                                  \n"," dropout_11 (Dropout)           (None, 4, 64, 80)    0           ['dense_11[0][0]']               \n","                                                                                                  \n"," add_14 (Add)                   (None, 4, 64, 80)    0           ['dropout_11[0][0]',             \n","                                                                  'add_13[0][0]']                 \n","                                                                                                  \n"," reshape_3 (Reshape)            (None, 16, 16, 80)   0           ['add_14[0][0]']                 \n","                                                                                                  \n"," conv2d_19 (Conv2D)             (None, 16, 16, 64)   5184        ['reshape_3[0][0]']              \n","                                                                                                  \n"," concatenate_1 (Concatenate)    (None, 16, 16, 128)  0           ['batch_normalization_17[0][0]', \n","                                                                  'conv2d_19[0][0]']              \n","                                                                                                  \n"," conv2d_20 (Conv2D)             (None, 16, 16, 80)   92240       ['concatenate_1[0][0]']          \n","                                                                                                  \n"," conv2d_21 (Conv2D)             (None, 16, 16, 160)  12800       ['conv2d_20[0][0]']              \n","                                                                                                  \n"," batch_normalization_18 (BatchN  (None, 16, 16, 160)  640        ['conv2d_21[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," tf.nn.silu_12 (TFOpLambda)     (None, 16, 16, 160)  0           ['batch_normalization_18[0][0]'] \n","                                                                                                  \n"," zero_padding2d_3 (ZeroPadding2  (None, 17, 17, 160)  0          ['tf.nn.silu_12[0][0]']          \n"," D)                                                                                               \n","                                                                                                  \n"," depthwise_conv2d_6 (DepthwiseC  (None, 8, 8, 160)   1440        ['zero_padding2d_3[0][0]']       \n"," onv2D)                                                                                           \n","                                                                                                  \n"," batch_normalization_19 (BatchN  (None, 8, 8, 160)   640         ['depthwise_conv2d_6[0][0]']     \n"," ormalization)                                                                                    \n","                                                                                                  \n"," tf.nn.silu_13 (TFOpLambda)     (None, 8, 8, 160)    0           ['batch_normalization_19[0][0]'] \n","                                                                                                  \n"," conv2d_22 (Conv2D)             (None, 8, 8, 80)     12800       ['tf.nn.silu_13[0][0]']          \n","                                                                                                  \n"," batch_normalization_20 (BatchN  (None, 8, 8, 80)    320         ['conv2d_22[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," conv2d_23 (Conv2D)             (None, 8, 8, 96)     69216       ['batch_normalization_20[0][0]'] \n","                                                                                                  \n"," conv2d_24 (Conv2D)             (None, 8, 8, 96)     9312        ['conv2d_23[0][0]']              \n","                                                                                                  \n"," reshape_4 (Reshape)            (None, 4, 16, 96)    0           ['conv2d_24[0][0]']              \n","                                                                                                  \n"," layer_normalization_12 (LayerN  (None, 4, 16, 96)   192         ['reshape_4[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," multi_head_attention_6 (MultiH  (None, 4, 16, 96)   74400       ['layer_normalization_12[0][0]', \n"," eadAttention)                                                    'layer_normalization_12[0][0]'] \n","                                                                                                  \n"," add_15 (Add)                   (None, 4, 16, 96)    0           ['multi_head_attention_6[0][0]', \n","                                                                  'reshape_4[0][0]']              \n","                                                                                                  \n"," layer_normalization_13 (LayerN  (None, 4, 16, 96)   192         ['add_15[0][0]']                 \n"," ormalization)                                                                                    \n","                                                                                                  \n"," dense_12 (Dense)               (None, 4, 16, 192)   18624       ['layer_normalization_13[0][0]'] \n","                                                                                                  \n"," dropout_12 (Dropout)           (None, 4, 16, 192)   0           ['dense_12[0][0]']               \n","                                                                                                  \n"," dense_13 (Dense)               (None, 4, 16, 96)    18528       ['dropout_12[0][0]']             \n","                                                                                                  \n"," dropout_13 (Dropout)           (None, 4, 16, 96)    0           ['dense_13[0][0]']               \n","                                                                                                  \n"," add_16 (Add)                   (None, 4, 16, 96)    0           ['dropout_13[0][0]',             \n","                                                                  'add_15[0][0]']                 \n","                                                                                                  \n"," layer_normalization_14 (LayerN  (None, 4, 16, 96)   192         ['add_16[0][0]']                 \n"," ormalization)                                                                                    \n","                                                                                                  \n"," multi_head_attention_7 (MultiH  (None, 4, 16, 96)   74400       ['layer_normalization_14[0][0]', \n"," eadAttention)                                                    'layer_normalization_14[0][0]'] \n","                                                                                                  \n"," add_17 (Add)                   (None, 4, 16, 96)    0           ['multi_head_attention_7[0][0]', \n","                                                                  'add_16[0][0]']                 \n","                                                                                                  \n"," layer_normalization_15 (LayerN  (None, 4, 16, 96)   192         ['add_17[0][0]']                 \n"," ormalization)                                                                                    \n","                                                                                                  \n"," dense_14 (Dense)               (None, 4, 16, 192)   18624       ['layer_normalization_15[0][0]'] \n","                                                                                                  \n"," dropout_14 (Dropout)           (None, 4, 16, 192)   0           ['dense_14[0][0]']               \n","                                                                                                  \n"," dense_15 (Dense)               (None, 4, 16, 96)    18528       ['dropout_14[0][0]']             \n","                                                                                                  \n"," dropout_15 (Dropout)           (None, 4, 16, 96)    0           ['dense_15[0][0]']               \n","                                                                                                  \n"," add_18 (Add)                   (None, 4, 16, 96)    0           ['dropout_15[0][0]',             \n","                                                                  'add_17[0][0]']                 \n","                                                                                                  \n"," layer_normalization_16 (LayerN  (None, 4, 16, 96)   192         ['add_18[0][0]']                 \n"," ormalization)                                                                                    \n","                                                                                                  \n"," multi_head_attention_8 (MultiH  (None, 4, 16, 96)   74400       ['layer_normalization_16[0][0]', \n"," eadAttention)                                                    'layer_normalization_16[0][0]'] \n","                                                                                                  \n"," add_19 (Add)                   (None, 4, 16, 96)    0           ['multi_head_attention_8[0][0]', \n","                                                                  'add_18[0][0]']                 \n","                                                                                                  \n"," layer_normalization_17 (LayerN  (None, 4, 16, 96)   192         ['add_19[0][0]']                 \n"," ormalization)                                                                                    \n","                                                                                                  \n"," dense_16 (Dense)               (None, 4, 16, 192)   18624       ['layer_normalization_17[0][0]'] \n","                                                                                                  \n"," dropout_16 (Dropout)           (None, 4, 16, 192)   0           ['dense_16[0][0]']               \n","                                                                                                  \n"," dense_17 (Dense)               (None, 4, 16, 96)    18528       ['dropout_16[0][0]']             \n","                                                                                                  \n"," dropout_17 (Dropout)           (None, 4, 16, 96)    0           ['dense_17[0][0]']               \n","                                                                                                  \n"," add_20 (Add)                   (None, 4, 16, 96)    0           ['dropout_17[0][0]',             \n","                                                                  'add_19[0][0]']                 \n","                                                                                                  \n"," reshape_5 (Reshape)            (None, 8, 8, 96)     0           ['add_20[0][0]']                 \n","                                                                                                  \n"," conv2d_25 (Conv2D)             (None, 8, 8, 80)     7760        ['reshape_5[0][0]']              \n","                                                                                                  \n"," concatenate_2 (Concatenate)    (None, 8, 8, 160)    0           ['batch_normalization_20[0][0]', \n","                                                                  'conv2d_25[0][0]']              \n","                                                                                                  \n"," conv2d_26 (Conv2D)             (None, 8, 8, 96)     138336      ['concatenate_2[0][0]']          \n","                                                                                                  \n"," conv2d_27 (Conv2D)             (None, 8, 8, 320)    31040       ['conv2d_26[0][0]']              \n","                                                                                                  \n"," global_average_pooling2d (Glob  (None, 320)         0           ['conv2d_27[0][0]']              \n"," alAveragePooling2D)                                                                              \n","                                                                                                  \n"," dense_18 (Dense)               (None, 2)            642         ['global_average_pooling2d[0][0]'\n","                                                                 ]                                \n","                                                                                                  \n","==================================================================================================\n","Total params: 1,306,658\n","Trainable params: 1,304,114\n","Non-trainable params: 2,544\n","__________________________________________________________________________________________________\n"]}],"source":["\n","def create_mobilevit(num_classes=2):\n","    inputs = keras.Input((image_size, image_size, 3))\n","    x = layers.Rescaling(scale=1.0 / 255)(inputs)\n","\n","    # Initial conv-stem -> MV2 block.\n","    x = conv_block(x, filters=16)\n","    x = inverted_residual_block(\n","        x, expanded_channels=16 * expansion_factor, output_channels=16\n","    )\n","\n","    # Downsampling with MV2 block.\n","    x = inverted_residual_block(\n","        x, expanded_channels=16 * expansion_factor, output_channels=24, strides=2\n","    )\n","    x = inverted_residual_block(\n","        x, expanded_channels=24 * expansion_factor, output_channels=24\n","    )\n","    x = inverted_residual_block(\n","        x, expanded_channels=24 * expansion_factor, output_channels=24\n","    )\n","\n","    # First MV2 -> MobileViT block.\n","    x = inverted_residual_block(\n","        x, expanded_channels=24 * expansion_factor, output_channels=48, strides=2\n","    )\n","    x = mobilevit_block(x, num_blocks=2, projection_dim=64)\n","\n","    # Second MV2 -> MobileViT block.\n","    x = inverted_residual_block(\n","        x, expanded_channels=64 * expansion_factor, output_channels=64, strides=2\n","    )\n","    x = mobilevit_block(x, num_blocks=4, projection_dim=80)\n","\n","    # Third MV2 -> MobileViT block.\n","    x = inverted_residual_block(\n","        x, expanded_channels=80 * expansion_factor, output_channels=80, strides=2\n","    )\n","    x = mobilevit_block(x, num_blocks=3, projection_dim=96)\n","    x = conv_block(x, filters=320, kernel_size=1, strides=1)\n","\n","    # Classification head.\n","    x = layers.GlobalAvgPool2D()(x)\n","    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n","\n","    return keras.Model(inputs, outputs)\n","\n","\n","mobilevit_xxs = create_mobilevit()\n","mobilevit_xxs.summary()"]},{"cell_type":"markdown","metadata":{"id":"VXg378cpR6eQ"},"source":["## Dataset preparation\n","\n","We will be using the\n","[`tf_flowers`](https://www.tensorflow.org/datasets/catalog/tf_flowers)\n","dataset to demonstrate the model. Unlike other Transformer-based architectures,\n","MobileViT uses a simple augmentation pipeline primarily because it has the properties\n","of a CNN."]},{"cell_type":"code","execution_count":7,"metadata":{"id":"Xmv4kA9pR6eS","executionInfo":{"status":"ok","timestamp":1658307159861,"user_tz":-120,"elapsed":4,"user":{"displayName":"Sagnik Gupta","userId":"06692511853419568091"}}},"outputs":[],"source":["batch_size = 32\n","auto = tf.data.AUTOTUNE\n","resize_bigger = 256\n","num_classes = 2\n","\n","\n","def preprocess_dataset(is_training=True):\n","    def _pp(image, label):\n","        if is_training:\n","            # Resize to a bigger spatial resolution and take the random\n","            # crops.\n","            image = tf.image.resize(image, (resize_bigger, resize_bigger))\n","            # image = tf.image.random_crop(image, (image_size, image_size, 3))\n","            image = tf.image.random_flip_left_right(image)\n","        else:\n","            image = tf.image.resize(image, (image_size, image_size))\n","        label = tf.one_hot(label, depth=num_classes)\n","        return image, label\n","\n","    return _pp\n","\n","\n","def prepare_dataset(dataset, is_training=True):\n","    if is_training:\n","        dataset = dataset.shuffle(batch_size * 10)\n","    dataset = dataset.map(preprocess_dataset(is_training), num_parallel_calls=auto)\n","    return dataset.batch(batch_size).prefetch(auto)\n"]},{"cell_type":"markdown","metadata":{"id":"X9KX6pXAR6eU"},"source":["The authors use a multi-scale data sampler to help the model learn representations of\n","varied scales. In this example, we discard this part."]},{"cell_type":"markdown","metadata":{"id":"7VJcA03mR6eX"},"source":["## Load and prepare the dataset"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qvEE43339e-d","outputId":"8e699356-6747-4b0a-cee0-5bf3c0664435","executionInfo":{"status":"ok","timestamp":1658307180824,"user_tz":-120,"elapsed":20966,"user":{"displayName":"Sagnik Gupta","userId":"06692511853419568091"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"kcfApxxnXabz","executionInfo":{"status":"ok","timestamp":1658307180825,"user_tz":-120,"elapsed":4,"user":{"displayName":"Sagnik Gupta","userId":"06692511853419568091"}}},"outputs":[],"source":["from PIL import Image\n","from io import BytesIO\n","import os\n","from PIL import ImageFile\n","ImageFile.LOAD_TRUNCATED_IMAGES = True"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IqgIT_nzWIRp","outputId":"a353c10d-80a6-4cfd-88cb-dbb4b2740c89","executionInfo":{"status":"ok","timestamp":1658313419877,"user_tz":-120,"elapsed":579,"user":{"displayName":"Sagnik Gupta","userId":"06692511853419568091"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Found 12140 files belonging to 2 classes.\n","Using 9712 files for training.\n"]}],"source":["train_ds = tf.keras.utils.image_dataset_from_directory(\n","  '/content/drive/MyDrive/Colab Notebooks/CSP Lab/Vision Transformer/data',\n","  validation_split=0.2,\n","  subset=\"training\",\n","  seed=123,\n","  #label_mode='binary',\n","  image_size=(image_size, image_size),\n","  batch_size=batch_size)"]},{"cell_type":"code","execution_count":28,"metadata":{"id":"GR-2D7sYYWM8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1658314525959,"user_tz":-120,"elapsed":670,"user":{"displayName":"Sagnik Gupta","userId":"06692511853419568091"}},"outputId":"05a405b3-9a01-43e6-e411-d85480e1b819"},"outputs":[{"output_type":"stream","name":"stdout","text":["Found 12140 files belonging to 2 classes.\n","Using 2428 files for validation.\n"]}],"source":["val_ds = tf.keras.utils.image_dataset_from_directory(\n","  '/content/drive/MyDrive/Colab Notebooks/CSP Lab/Vision Transformer/data',\n","  validation_split=0.2,\n","  subset=\"validation\",\n","  seed=123,\n","  #label_mode='binary',\n","  image_size=(image_size, image_size),\n","  batch_size=batch_size)"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"Mqu7_HtFYkUW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1658307212548,"user_tz":-120,"elapsed":10,"user":{"displayName":"Sagnik Gupta","userId":"06692511853419568091"}},"outputId":"a0ea5caf-f61a-4686-e77b-5e6e2c8673c6"},"outputs":[{"output_type":"stream","name":"stdout","text":["['private', 'public']\n"]}],"source":["class_names = val_ds.class_names\n","print(class_names)"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"06BKMDEGcLZt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1658307212548,"user_tz":-120,"elapsed":6,"user":{"displayName":"Sagnik Gupta","userId":"06692511853419568091"}},"outputId":"f606427d-521d-423d-e452-cd2664e8ab50"},"outputs":[{"output_type":"stream","name":"stdout","text":["2.8.0\n"]}],"source":["print(tf. __version__)"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"FfPzguIJqzBH","executionInfo":{"status":"ok","timestamp":1658307212548,"user_tz":-120,"elapsed":5,"user":{"displayName":"Sagnik Gupta","userId":"06692511853419568091"}}},"outputs":[],"source":["# example = list(val_ds.as_numpy_iterator())\n","# example[0][0][3]"]},{"cell_type":"code","source":["# np.set_printoptions(threshold=1000)\n","# print(example[0][0][0][0])"],"metadata":{"id":"P-7jdd6M4fpl","executionInfo":{"status":"ok","timestamp":1658307212549,"user_tz":-120,"elapsed":5,"user":{"displayName":"Sagnik Gupta","userId":"06692511853419568091"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["# len(example[0][0])"],"metadata":{"id":"e6Y3-F9tT-0N","executionInfo":{"status":"ok","timestamp":1658307212549,"user_tz":-120,"elapsed":5,"user":{"displayName":"Sagnik Gupta","userId":"06692511853419568091"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","execution_count":17,"metadata":{"id":"VCFsVoL41GTI","executionInfo":{"status":"ok","timestamp":1658307212549,"user_tz":-120,"elapsed":5,"user":{"displayName":"Sagnik Gupta","userId":"06692511853419568091"}}},"outputs":[],"source":["# from PIL import Image\n","# import numpy as np\n","# import glob\n","\n","# # images = []\n","# # dir = '/content/drive/MyDrive/Colab Notebooks/CSP Lab/Vision Transformer/data/public/*'\n","\n","# # for f in glob.iglob(dir):\n","# #   images = []\n","# #   try:\n","# #     images.append(np.asarray(Image.open(f)))\n","# #   except:\n","# #     print('kaputt: ', f)  \n","\n","# # #images = np.array(images)"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"7kbTM4TNF-IW","executionInfo":{"status":"ok","timestamp":1658307212549,"user_tz":-120,"elapsed":4,"user":{"displayName":"Sagnik Gupta","userId":"06692511853419568091"}}},"outputs":[],"source":["# from struct import unpack\n","# from tqdm import tqdm\n","# import os\n","\n","\n","# marker_mapping = {\n","#     0xffd8: \"Start of Image\",\n","#     0xffe0: \"Application Default Header\",\n","#     0xffdb: \"Quantization Table\",\n","#     0xffc0: \"Start of Frame\",\n","#     0xffc4: \"Define Huffman Table\",\n","#     0xffda: \"Start of Scan\",\n","#     0xffd9: \"End of Image\"\n","# }\n","\n","\n","# class JPEG:\n","#     def __init__(self, image_file):\n","#         with open(image_file, 'rb') as f:\n","#             self.img_data = f.read()\n","    \n","#     def decode(self):\n","#         data = self.img_data\n","#         while(True):\n","#             marker, = unpack(\">H\", data[0:2])\n","#             # print(marker_mapping.get(marker))\n","#             if marker == 0xffd8:\n","#                 data = data[2:]\n","#             elif marker == 0xffd9:\n","#                 return\n","#             elif marker == 0xffda:\n","#                 data = data[-2:]\n","#             else:\n","#                 lenchunk, = unpack(\">H\", data[2:4])\n","#                 data = data[2+lenchunk:]            \n","#             if len(data)==0:\n","#                 break        \n","\n","\n","# bads = []\n","# dir = '/content/drive/MyDrive/Colab Notebooks/CSP Lab/Vision Transformer/data/private/*'\n","\n","# for imagename in glob.iglob(dir):\n","#   image = JPEG(imagename) \n","#   try:\n","#     image.decode()   \n","#   except:\n","#     bads.append(imagename)"]},{"cell_type":"markdown","metadata":{"id":"ZiHO9RfCR6ed"},"source":["## Train a MobileViT (XXS) model"]},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ovYCRUPyR6ef","outputId":"d16e2395-7b79-4b86-950d-a9044dc2b789","executionInfo":{"status":"ok","timestamp":1658317101776,"user_tz":-120,"elapsed":2570471,"user":{"displayName":"Sagnik Gupta","userId":"06692511853419568091"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/15\n","304/304 [==============================] - 165s 489ms/step - loss: 0.6664 - accuracy: 0.5910 - val_loss: 0.6947 - val_accuracy: 0.4971\n","Epoch 2/15\n","304/304 [==============================] - 147s 482ms/step - loss: 0.6243 - accuracy: 0.6552 - val_loss: 0.7305 - val_accuracy: 0.5305\n","Epoch 3/15\n","304/304 [==============================] - 147s 482ms/step - loss: 0.5757 - accuracy: 0.7007 - val_loss: 0.6851 - val_accuracy: 0.6561\n","Epoch 4/15\n","304/304 [==============================] - 147s 480ms/step - loss: 0.5061 - accuracy: 0.7475 - val_loss: 0.7475 - val_accuracy: 0.6656\n","Epoch 5/15\n","304/304 [==============================] - 147s 480ms/step - loss: 0.3961 - accuracy: 0.8122 - val_loss: 0.9220 - val_accuracy: 0.6454\n","Epoch 6/15\n","304/304 [==============================] - 146s 476ms/step - loss: 0.2928 - accuracy: 0.8725 - val_loss: 1.0664 - val_accuracy: 0.6582\n","Epoch 7/15\n","304/304 [==============================] - 146s 479ms/step - loss: 0.2225 - accuracy: 0.9085 - val_loss: 1.1142 - val_accuracy: 0.6635\n","Epoch 8/15\n","304/304 [==============================] - 146s 479ms/step - loss: 0.1863 - accuracy: 0.9224 - val_loss: 1.2814 - val_accuracy: 0.6635\n","Epoch 9/15\n","304/304 [==============================] - 146s 479ms/step - loss: 0.1455 - accuracy: 0.9400 - val_loss: 1.4249 - val_accuracy: 0.6598\n","Epoch 10/15\n","304/304 [==============================] - 146s 478ms/step - loss: 0.1206 - accuracy: 0.9502 - val_loss: 1.5915 - val_accuracy: 0.6676\n","Epoch 11/15\n","304/304 [==============================] - 147s 479ms/step - loss: 0.1101 - accuracy: 0.9584 - val_loss: 1.6978 - val_accuracy: 0.6462\n","Epoch 12/15\n","304/304 [==============================] - 146s 479ms/step - loss: 0.1023 - accuracy: 0.9578 - val_loss: 1.4453 - val_accuracy: 0.6623\n","Epoch 13/15\n","304/304 [==============================] - 146s 478ms/step - loss: 0.0914 - accuracy: 0.9661 - val_loss: 1.5975 - val_accuracy: 0.6384\n","Epoch 14/15\n","304/304 [==============================] - 146s 479ms/step - loss: 0.0805 - accuracy: 0.9686 - val_loss: 1.6203 - val_accuracy: 0.6557\n","Epoch 15/15\n","304/304 [==============================] - 146s 478ms/step - loss: 0.0638 - accuracy: 0.9752 - val_loss: 1.6353 - val_accuracy: 0.6536\n","76/76 [==============================] - 11s 128ms/step - loss: 1.5915 - accuracy: 0.6676\n","Validation accuracy: 66.76%\n"]}],"source":["learning_rate = 0.0001\n","label_smoothing_factor = 0.1\n","epochs = 15\n","num_classes = 2\n","\n","optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n","#loss_fn = keras.losses.BinaryCrossentropy(label_smoothing=label_smoothing_factor)\n","loss_fn = keras.losses.SparseCategoricalCrossentropy()\n","\n","def run_experiment(epochs=epochs):\n","    mobilevit_xxs = create_mobilevit(num_classes=num_classes)\n","    mobilevit_xxs.compile(optimizer=optimizer, loss=loss_fn, metrics=[\"accuracy\"])\n","\n","    checkpoint_filepath = \"/tmp/checkpoint\"\n","    checkpoint_callback = keras.callbacks.ModelCheckpoint(\n","        checkpoint_filepath,\n","        monitor=\"val_accuracy\",\n","        save_best_only=True,\n","        save_weights_only=True,\n","    )\n","\n","    mobilevit_xxs.fit(\n","        train_ds,\n","        validation_data=val_ds,\n","        epochs=epochs,\n","        callbacks=[checkpoint_callback], \n","    )   \n","    mobilevit_xxs.load_weights(checkpoint_filepath)\n","    _, accuracy = mobilevit_xxs.evaluate(val_ds)\n","    print(f\"Validation accuracy: {round(accuracy * 100, 2)}%\")\n","    return mobilevit_xxs\n","\n","\n","mobilevit_xxs = run_experiment()"]},{"cell_type":"code","source":["val_ds = tf.keras.utils.image_dataset_from_directory(\n","  '/content/drive/MyDrive/Colab Notebooks/CSP Lab/Vision Transformer/data3',\n","  # validation_split=0.95,\n","  # subset=\"validation\",\n","  # seed=123,\n","  label_mode='binary',\n","  image_size=(image_size, image_size),\n","  batch_size=batch_size)"],"metadata":{"id":"1dTzoOcMsCHn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1658318353401,"user_tz":-120,"elapsed":233,"user":{"displayName":"Sagnik Gupta","userId":"06692511853419568091"}},"outputId":"21665ef6-e8a8-41ca-cb57-85e2b010e8a0"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 48 files belonging to 2 classes.\n"]}]},{"cell_type":"code","source":["_, accuracy = mobilevit_xxs.evaluate(val_ds)\n","print(f\"Validation accuracy: {round(accuracy * 100, 2)}%\")"],"metadata":{"id":"gJCeqKY_sh49","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1658318360182,"user_tz":-120,"elapsed":5276,"user":{"displayName":"Sagnik Gupta","userId":"06692511853419568091"}},"outputId":"3cf1cc93-daa4-4284-d130-b8d7ad3a1a7d"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["2/2 [==============================] - 4s 81ms/step - loss: 0.2147 - accuracy: 0.9583\n","Validation accuracy: 95.83%\n"]}]},{"cell_type":"markdown","metadata":{"id":"TNSKe3tMR6ei"},"source":["## Results and TFLite conversion\n","\n","With about one million parameters, getting to ~85% top-1 accuracy on 256x256 resolution is\n","a strong result. This MobileViT mobile is fully compatible with TensorFlow Lite (TFLite)\n","and can be converted with the following code:"]},{"cell_type":"code","execution_count":35,"metadata":{"id":"enm3m4i8R6em","executionInfo":{"status":"ok","timestamp":1658318437832,"user_tz":-120,"elapsed":49047,"user":{"displayName":"Sagnik Gupta","userId":"06692511853419568091"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"3eba733a-ec11-470b-8b3c-a69240daec2d"},"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:absl:Found untraced functions such as query_layer_call_fn, query_layer_call_and_return_conditional_losses, key_layer_call_fn, key_layer_call_and_return_conditional_losses, value_layer_call_fn while saving (showing 5 of 108). These functions will not be directly callable after loading.\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:Assets written to: mobilevit_xxs/assets\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:Assets written to: mobilevit_xxs/assets\n","WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded\n"]},{"output_type":"execute_result","data":{"text/plain":["1672448"]},"metadata":{},"execution_count":35}],"source":["# Serialize the model as a SavedModel.\n","mobilevit_xxs.save(\"mobilevit_xxs\")\n","\n","# Convert to TFLite. This form of quantization is called\n","# post-training dynamic-range quantization in TFLite.\n","converter = tf.lite.TFLiteConverter.from_saved_model(\"mobilevit_xxs\")\n","converter.optimizations = [tf.lite.Optimize.DEFAULT]\n","converter.target_spec.supported_ops = [\n","    tf.lite.OpsSet.TFLITE_BUILTINS,  # Enable TensorFlow Lite ops.\n","    tf.lite.OpsSet.SELECT_TF_OPS,  # Enable TensorFlow ops.\n","]\n","tflite_model = converter.convert()\n","open(\"mobilevit_xxs.tflite\", \"wb\").write(tflite_model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mtg-t7AZLJ8d"},"outputs":[],"source":["# # Copy the model files to a directory in your Google Drive.\n","# from google.colab import drive\n","# drive.mount('/content/drive')\n","# !cp -r \"./mobilevit_xxs\" \"./drive/My Drive/Colab Notebooks/mobilevit_model/\""]},{"cell_type":"markdown","metadata":{"id":"VxYQyqMQR6ep"},"source":["To learn more about different quantization recipes available in TFLite and running\n","inference with TFLite models, check out\n","[this official resource](https://www.tensorflow.org/lite/performance/post_training_quantization).\n","\n","You can use the trained model hosted on [Hugging Face Hub](https://huggingface.co/keras-io/mobile-vit-xxs) and try the demo on [Hugging Face Spaces](https://huggingface.co/spaces/keras-io/Flowers-Classification-MobileViT)."]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"mobilevit_binaryLoss.ipynb","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"}},"nbformat":4,"nbformat_minor":0}